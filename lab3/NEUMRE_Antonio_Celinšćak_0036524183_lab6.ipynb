{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Jw6CDFlQYxtx"
      },
      "source": [
        "# 6 Support Vector Machine\n",
        "## 6.1 Kernels\n",
        "Support Vector Machine can [use different kernels](https://en.wikipedia.org/wiki/Kernel_method): linear, radial basis function, polynomial, sigmoid, etc. The difference between some of them can be seen after running the code below that uses a classical example. Besides the usual packages, the *sklearn* package is also used here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV9mIY0QYxty"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "#take the well-known iris dataset\n",
        "iris = datasets.load_iris()\n",
        "#we will use only sepal length and width\n",
        "x=iris.data[:, :2]\n",
        "y=iris.target\n",
        "\n",
        "#plot points\n",
        "x1, x2=x[:, 0], x[:, 1]\n",
        "x_min, x_max=x1.min()-1, x1.max()+1\n",
        "y_min, y_max=x2.min()-1, x2.max()+1\n",
        "h=0.02\n",
        "plot_x, plot_y=np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "#regularization\n",
        "C=1.0  \n",
        "models=(svm.SVC(kernel=\"linear\", C=C),\n",
        "          svm.SVC(kernel=\"rbf\", gamma=0.7, C=C),\n",
        "          svm.SVC(kernel=\"poly\", degree=3, C=C))\n",
        "models=(model.fit(x, y) for model in models)\n",
        "\n",
        "# title for the plots\n",
        "titles = (\"Linear kernel\", \"RBF kernel\", \"Polynomial (degree 3) kernel\")\n",
        "\n",
        "\n",
        "for model, title in zip(models, titles):\n",
        "    points=model.predict(np.c_[plot_x.ravel(), plot_y.ravel()]).reshape(plot_x.shape)\n",
        "    plt.contourf(plot_x, plot_y, points, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "    plt.xlim(plot_x.min(), plot_x.max())\n",
        "    plt.ylim(plot_y.min(), plot_y.max())\n",
        "    plt.xlabel(\"Sepal length\")\n",
        "    plt.ylabel(\"Sepal width\")\n",
        "    plt.title(title)\n",
        "    \n",
        "    predicted=model.predict(x);\n",
        "    print(\"Accuracy: %.2lf%%\"%(100*np.sum(y==predicted)/y.size))\n",
        "    \n",
        "    plt.scatter(x1, x2, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxjnkNBHYxt0"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. What accuracies are achieved when other features are used as well?\n",
        "2. Split the dataset into a training and testing part, fit the SVM model on the training part, and test it on the testing part. What gives the highest accuracy?\n",
        "3. Make the code below give over 90% accuracy and then explain how you achieved it and why did it work."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. zadatak\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "#take the well-known iris dataset\n",
        "iris = datasets.load_iris()\n",
        "#we will use only sepal length and width\n",
        "x=iris.data[:, :2]\n",
        "y=iris.target\n",
        "\n",
        "\n",
        "#regularization\n",
        "C=1.0  \n",
        "models=(svm.SVC(kernel=\"linear\", C=C),\n",
        "          svm.SVC(kernel=\"rbf\", gamma=0.7, C=C),\n",
        "          svm.SVC(kernel=\"poly\", degree=3, C=C))\n",
        "models=(model.fit(x, y) for model in models)\n",
        "\n",
        "# title for the plots\n",
        "titles = (\"Linear kernel\", \"RBF kernel\", \"Polynomial (degree 3) kernel\")\n",
        "\n",
        "for model, title in zip(models, titles):\n",
        "\n",
        "    predicted=model.predict(x);\n",
        "    print(\"Za jezgru \"+ title, end=\"\")\n",
        "    print(\" accuracy: %.2lf%%\"%(100*np.sum(y==predicted)/y.size))"
      ],
      "metadata": {
        "id": "2YLCBNZCo6Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. zadatak\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#take the well-known iris dataset\n",
        "iris = datasets.load_iris()\n",
        "#we will use only sepal length and width\n",
        "x=iris.data[]\n",
        "y=iris.target\n",
        "\n",
        "#split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "#regularization\n",
        "C=1.0  \n",
        "models=(svm.SVC(kernel=\"linear\", C=C),\n",
        "          svm.SVC(kernel=\"rbf\", gamma=0.7, C=C),\n",
        "          svm.SVC(kernel=\"poly\", degree=3, C=C))\n",
        "models=(model.fit(x_train, y_train) for model in models)\n",
        "\n",
        "# title for the plots\n",
        "titles = (\"Linear kernel\", \"RBF kernel\", \"Polynomial (degree 3) kernel\")\n",
        "\n",
        "for model, title in zip(models, titles):\n",
        "\n",
        "    predicted=model.predict(x_test);\n",
        "    print(\"Za jezgru \"+ title, end=\"\")\n",
        "    print(\" accuracy: %.2lf%%\"%(100*np.sum(y_test==predicted)/y_test.size))"
      ],
      "metadata": {
        "id": "dW9g35-OqIJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4CnRHKGYxt0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "n1=400\n",
        "n2=400\n",
        "\n",
        "class1=(np.tile(np.random.uniform(low=0.0, high=1, size=n2).reshape((n2, 1)), (1, 2))+3/2)*\\\n",
        "np.array([(np.cos(a), np.sin(a)) for a in np.random.uniform(low=2, high=8, size=n2)])+np.tile(np.array([[3/2, 0]]), (n1, 1))\n",
        "class2=(np.tile(np.random.uniform(low=0.0, high=1, size=n2).reshape((n2, 1)), (1, 2))+3/2)*\\\n",
        "np.array([(np.cos(a), np.sin(a)) for a in np.random.uniform(low=-1, high=4, size=n2)])\n",
        "x=np.vstack((class1, class2))\n",
        "y=np.concatenate((np.ones((n1)), 2*np.ones((n2))))\n",
        "\n",
        "idx=np.random.permutation(y.size)\n",
        "x=x[idx, :]\n",
        "y=y[idx]\n",
        "\n",
        "s=round((n1+n2)/2)\n",
        "#s=600\n",
        "\n",
        "x_train=x[:s, :]\n",
        "y_train=y[:s]\n",
        "\n",
        "x_test=x[s:, :]\n",
        "y_test=y[s:]\n",
        "\n",
        "#EDIT ONLY FROM HERE...\n",
        "model=svm.SVC(kernel=\"rbf\")\n",
        "model.fit(x_train, y_train)\n",
        "#...TO HERE\n",
        "\n",
        "predicted=model.predict(x_test);\n",
        "print(\"Accuracy: %.2lf%%\"%(100*np.sum(y_test==predicted)/y_test.size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Promjenom jezgre na rbf točnost se podigla iznad 90%. To je radilo jer rbf jezgra može bolje definirati granice tj mogu biti ukošene, a ne samo ravne kao kod linearne."
      ],
      "metadata": {
        "id": "oCsMtq9uq9xR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRA4HylFYxt0"
      },
      "source": [
        "## 6.2 Wine dataset\n",
        "Here we are going to make some experiments with the wine dataset to see how features can [affect](https://en.wikipedia.org/wiki/Feature_selection) the classification.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Which SVM kernel will achieve the highest accuracy when all features are used?\n",
        "\n",
        "Linearna jezgra\n",
        "\n",
        "2. If you can use **only one** feature and any kernel to achieve highest possible accuracy, which feature and kernel would that be?\n",
        "\n",
        "Sedmi feature(najčešće najveći) i linearnu jezgru.\n",
        "\n",
        "3. If you can use **only two** features and any kernel to achieve highest possible accuracy, which feature and kernel would that be?\n",
        "\n",
        "Deseti i sedmi jer su ta 2 najćešće najveći i linearnu jezgru\n",
        "4. How do you explain the results?\n",
        "\n",
        "Za težine granica največi zbroj za sve tri granice donosi najveći zančaj te značajke jer za nebitne značajke nisu visoke težine jer te težine ne utječu na klasifikaciju.\n",
        "\n",
        "Testiranje u donjem kodu pokazuje da je pretpostavka za 1 znacajku dobro, ali za dvije da dolazi do razlicitih kombinacija, najcesce koristeci RBF jezgru."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCXdI_JwYxt1"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "wine=load_wine()\n",
        "x=wine.data\n",
        "y=wine.target\n",
        "idx=np.random.permutation(y.size)\n",
        "x=x[idx, :]\n",
        "y=y[idx]\n",
        "\n",
        "#all features\n",
        "features_idx=range(x.shape[1])\n",
        "#only some of the features\n",
        "#features_idx=[0, 1]\n",
        "\n",
        "x=x[:, features_idx]\n",
        "\n",
        "s=round(y.size/2)\n",
        "\n",
        "x_train=x[:s, :]\n",
        "y_train=y[:s]\n",
        "\n",
        "x_test=x[s:, :]\n",
        "y_test=y[s:]\n",
        "\n",
        "models=(svm.SVC(kernel=\"linear\"),\n",
        "          svm.SVC(kernel=\"rbf\"),\n",
        "          svm.SVC(kernel=\"poly\", degree=3))\n",
        "models=(model.fit(x_train, y_train) for model in models)\n",
        "\n",
        "titles = (\"Linear kernel\", \"RBF kernel\", \"Polynomial (degree 3) kernel\")\n",
        "\n",
        "i=1\n",
        "tezine=[]\n",
        "for model, title in zip(models, titles):\n",
        "\n",
        "    if i==1:\n",
        "      i=0\n",
        "      tezine=model.coef_\n",
        "\n",
        "    predicted=model.predict(x_test);\n",
        "    print(\"Za jezgru \"+ title, end=\"\")\n",
        "    print(\" accuracy: %.2lf%%\"%(100*np.sum(y_test==predicted)/y_test.size))\n",
        "\n",
        "sume_tezina=[]\n",
        "for w1, w2, w3 in zip(tezine[0], tezine[1], tezine[2]):\n",
        "    sume_tezina.append(abs(w1)+abs(w2)+abs(w3))\n",
        "\n",
        "i=1\n",
        "for suma in sume_tezina:\n",
        "  print(str(i)+\". feature has sum of weights equal to \"+ str(suma))\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. zadatak\n",
        "from sklearn.datasets import load_wine\n",
        "wine=load_wine()\n",
        "\n",
        "\n",
        "#all features\n",
        "#features_idx=range(x.shape[1])\n",
        "#only some of the features\n",
        "\n",
        "max_acc=0\n",
        "max_model=\"\"\n",
        "max_znacajka=0\n",
        "\n",
        "for feature in range(0, 12):\n",
        "  x=wine.data\n",
        "  y=wine.target\n",
        "  idx=np.random.permutation(y.size)\n",
        "  x=x[idx, :]\n",
        "  y=y[idx]\n",
        "\n",
        "  features_idx=[feature]\n",
        "\n",
        "  x=x[:, features_idx]\n",
        "\n",
        "  s=round(y.size/2)\n",
        "\n",
        "  x_train=x[:s, :]\n",
        "  y_train=y[:s]\n",
        "\n",
        "  x_test=x[s:, :]\n",
        "  y_test=y[s:]\n",
        "\n",
        "  models=(svm.SVC(kernel=\"linear\"),\n",
        "            svm.SVC(kernel=\"rbf\"),\n",
        "            svm.SVC(kernel=\"poly\", degree=3))\n",
        "  models=(model.fit(x_train, y_train) for model in models)\n",
        "\n",
        "  titles = (\"Linear kernel\", \"RBF kernel\", \"Polynomial (degree 3) kernel\")\n",
        "\n",
        "  for model, title in zip(models, titles):\n",
        "      predicted=model.predict(x_test);\n",
        "      if (100*np.sum(y_test==predicted)/y_test.size)>max_acc:\n",
        "          max_acc=(100*np.sum(y_test==predicted)/y_test.size)\n",
        "          max_model=title\n",
        "          max_znacajka=feature\n",
        "\n",
        "print(\"Za jezgru \"+ max_model+ \" i znacajku \"+ str(max_znacajka+1), end=\"\")\n",
        "print(\" accuracy: \"+ str(max_acc))"
      ],
      "metadata": {
        "id": "QGvM4Wru1B5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. zadatak\n",
        "from sklearn.datasets import load_wine\n",
        "wine=load_wine()\n",
        "\n",
        "\n",
        "#all features\n",
        "#features_idx=range(x.shape[1])\n",
        "#only some of the features\n",
        "\n",
        "max_acc=0\n",
        "max_model=\"\"\n",
        "max_znacajka1=0\n",
        "max_znacajka2=0\n",
        "for feature1 in range(0, 12):\n",
        "\n",
        "  for feature2 in range(0, 12):\n",
        "    if feature1== feature2:\n",
        "      continue\n",
        "\n",
        "    x=wine.data\n",
        "    y=wine.target\n",
        "    idx=np.random.permutation(y.size)\n",
        "    x=x[idx, :]\n",
        "    y=y[idx]\n",
        "\n",
        "    features_idx=[feature1, feature2]\n",
        "\n",
        "    x=x[:, features_idx]\n",
        "\n",
        "    s=round(y.size/2)\n",
        "\n",
        "    x_train=x[:s, :]\n",
        "    y_train=y[:s]\n",
        "\n",
        "    x_test=x[s:, :]\n",
        "    y_test=y[s:]\n",
        "\n",
        "    models=(svm.SVC(kernel=\"linear\"),\n",
        "              svm.SVC(kernel=\"rbf\"),\n",
        "              svm.SVC(kernel=\"poly\", degree=3))\n",
        "    models=(model.fit(x_train, y_train) for model in models)\n",
        "\n",
        "    titles = (\"Linear kernel\", \"RBF kernel\", \"Polynomial (degree 3) kernel\")\n",
        "\n",
        "    for model, title in zip(models, titles):\n",
        "        predicted=model.predict(x_test);\n",
        "        if (100*np.sum(y_test==predicted)/y_test.size)>max_acc:\n",
        "            max_acc=(100*np.sum(y_test==predicted)/y_test.size)\n",
        "            max_model=title\n",
        "            max_znacajka1=feature1\n",
        "            max_znacajka2=feature2\n",
        "\n",
        "print(\"Za jezgru \"+ max_model+ \" i znacajke \"+ str(max_znacajka1+1)+ \" i \"+ str(max_znacajka2+1), end=\"\")\n",
        "print(\": accuracy: \"+ str(max_acc))"
      ],
      "metadata": {
        "id": "YaV6hjNj1zPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq_6am96Yxt1"
      },
      "source": [
        "## 6.3 Speed\n",
        "SVM is really great, but it has an important disadvantage with respect to neural networks in general. Here we are going to demonstrate it.\n",
        "\n",
        "**Tasks**\n",
        "1. Run the code below for various dataset sizes and each time store the time needed for the model to fit.\n",
        "2. Draw a plot that shows the influence of dataset size on execution time.\n",
        "3. How would you model the influence?\n",
        "\n",
        "Eksponencijalnom funkcijom\n",
        "\n",
        "4. How would you model the same influence in case of multilayer perceptron?\n",
        "Vjerojatno nekom polinomijalnom ili linearnom funkcijom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHlhlYXbYxt1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "def create_data(n1, n2):\n",
        "    class1=np.c_[np.random.normal(0, 1, size=n1), np.random.normal(0, 1, size=n1)]\n",
        "    class2=np.c_[np.random.normal(2, 1, size=n2), np.random.normal(0, 1, size=n2)]\n",
        "    x=np.vstack((class1, class2))\n",
        "    y=np.concatenate((np.ones((n1)), 2*np.ones((n2))))\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "times=[]\n",
        "data_sizes=[1000, 5000, 10000, 50000, 100000]\n",
        "for data_size in data_sizes:\n",
        "  x, y=create_data(data_size, data_size)\n",
        "  model=svm.SVC(kernel=\"linear\", C=1.0)\n",
        "  import time;\n",
        "  start=time.time()\n",
        "  model.fit(x, y)\n",
        "  end=time.time();\n",
        "  t=end-start\n",
        "  times.append(t)\n",
        "\n",
        "plt.plot(data_sizes, times)  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}