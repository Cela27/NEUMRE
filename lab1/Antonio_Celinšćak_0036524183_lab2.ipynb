{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5spXk1ZkEp6o"
      },
      "source": [
        "# Exercise 2: Associative memory\n",
        "\n",
        "## 2.1 Forming the correlation matrix directly\n",
        "\n",
        "In this part of the exercise we will use the direct approach in forming the correlation matrix. Memory based on the correlation matrix should memorize input-output association pairs represented as vectors. For each input vector (key) the memory has to memorize the output pattern i.e. vector in an ASCII code formulation. In this example we will use 4-dimensional input and output vectors. Words (output vectos) that have to be memorized are: '*vrat*' , '*kraj*' , '*cres*' , '*otac*'. Vectors $b_i$, which represent those words should be formed as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X3tn9IGbEp6p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "real=lambda x: np.array([[ord(character) for character in x]]).T\n",
        "\n",
        "\n",
        "b1=real(\"vrat\")\n",
        "b2=real(\"kraj\")\n",
        "b3=real(\"cres\")\n",
        "b4=real(\"otac\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sccA_iAoEp6q"
      },
      "source": [
        "### 2.1.1 Orthogonal input vectors\n",
        "\n",
        "This experiment demonstrates how to create an associative memory. Ortonormalized set of vectors defined as below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HblQuVjhEp6q"
      },
      "outputs": [],
      "source": [
        "a1 = np.array([[1, 0, 0, 0]]).T\n",
        "a2 = np.array([[0, 1, 0, 0]]).T\n",
        "a3 = np.array([[0, 0, 1, 0]]).T\n",
        "a4 = np.array([[0, 0, 0, 1]]).T\n",
        "\n",
        "#a1 = np.array([[1, 1, 0, 0]]).T\n",
        "#a2 = np.array([[0, 1, 0, 0]]).T\n",
        "#a3 = np.array([[0, 0, 1, 0]]).T\n",
        "#a4 = np.array([[0, 0, 0, 1]]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCbyFipSEp6r"
      },
      "source": [
        "is used as input vector set (set of keys). We form the memory correlation matrix $\\mathbf{M}$ using input output pairs as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ280RefEp6r"
      },
      "outputs": [],
      "source": [
        "M = b1 * a1.T + b2 * a2.T + b3 * a3.T + b4 * a4.T\n",
        "print(M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ugqVDk6Ep6r"
      },
      "source": [
        "In order to verify whether the memory is functioning properly, we have to calculate outputs for each input vector. For example, the output for the key $a_1$ can be obtained as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrkORfvnEp6r"
      },
      "outputs": [],
      "source": [
        "char=lambda x:\"\".join(map(chr, map(int, list(x))))\n",
        "\n",
        "word=char(M@a1)\n",
        "print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrfLjCW6Ep6r"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. What is the response for each key? Were all input-output pairs memorized correctly?\n",
        "2. How many input-output pairs would be memorized if vectors $a_i$ were not normalized?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-noL9AtEp6s"
      },
      "outputs": [],
      "source": [
        "for k in [a1, a2, a3, a4]:\n",
        "    print(k.T, char(M@k))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mheDsn3LfqlK"
      },
      "source": [
        "Sve riječi su dobro zapamćene. Ako promjenimo matricu i onda pokušamo ponoviti postupak nećemo dobiti spremljlene riječi nego niz raznih simbola poput:\n",
        "\n",
        "[[1 1 1 0]] ̠̊ʫ̆\n",
        "\n",
        "[[0 1 1 0]] ʔʮɊʒ\n",
        "\n",
        "[[0 0 1 0]] ńŖħő\n",
        "\n",
        "[[0 1 0 1]] ƿǌƄƤ\n",
        "\n",
        "Svejedno pamti 4 para ulaz izlaz ali ih ne pamti dobro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5QRnf5iEp6s"
      },
      "source": [
        "### 2.1.2 Correlation matrix properties\n",
        "\n",
        "The goal of this experiment is to demonstrate the capacity of obtained memory. In this part of the exercise we will try to memorize one more (fifth) word ('*mrak*'). In 4-dimensional vector space the maximum number of linearly independent vectors is four. Because of this fact, we pick an arbitrary unit vector as the fifth key, for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a4cp9kf4Ep6s"
      },
      "outputs": [],
      "source": [
        "a5 = (a1 + a3) / np.sqrt(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOX_J9OiEp6s"
      },
      "source": [
        "Form vectors $b_5$ ('*mrak*') and $a_5$ as explained and add them into the memory using the following expression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1V3Pk0j7Ep6s"
      },
      "outputs": [],
      "source": [
        "b5 = real(\"mrak\")\n",
        "M_five = b1 * a1.T + b2 * a2.T + b3 * a3.T + b4 * a4.T + b5 * a5.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OqtNHo7Ep6t"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. Was the new association properly memorized?\n",
        "2. Did other associations stay correctly memorized?\n",
        "    - If not - which were not memorized correctly and why?\n",
        "    - If yes - which were memorized correctly and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udiun0W8Ep6t"
      },
      "outputs": [],
      "source": [
        "for k in [a1, a2, a3, a4]:\n",
        "    before=char(M@k)\n",
        "    after=char(M_five@k)\n",
        "    print(k.T, before, after)\n",
        "\n",
        "print(a5.T, char(M_five@a5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgaQChTbpLyD"
      },
      "source": [
        "Riječ mrak nije bila dobro memorizirana zato jer redci u koju smo ju htjeli zapisati već sadrže informacije za riječi vrat i cres. Zbog toga što smo pokušali dodati u retke dodatne informacije izgubili smko u matrici M_five i prethodno pohranjene riječi. U redcima koje nismo dirali riječi kraj i otac su ostale zapamćene."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ew3_GrEp6t"
      },
      "source": [
        "### 2.1.3 Word pairs as associations\n",
        "\n",
        "In this experiment we will form the associative memory, which memorizes word pairs. The associations, which have to be memorized are: *ruka*-*vrat*, *kset*-*kraj*, *more*-*cres*, *mama*-*otac*. Generate input vectors (keys) as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3UUHiYnEp6t"
      },
      "outputs": [],
      "source": [
        "a1 = real(\"ruka\")\n",
        "a2 = real(\"kset\")\n",
        "a3 = real(\"more\")\n",
        "a4 = real(\"mama\")\n",
        "\n",
        "M = b1 * a1.T + b2 * a2.T + b3 * a3.T + b4 * a4.T\n",
        "print(M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj09n6RiEp6t"
      },
      "source": [
        "Vectors bi don't have to be created again because they are the ones used in the first part of the exercise. Form the matrix M using the same procedure as in the first part of the exercise.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. What is the response for each input key?\n",
        "\n",
        "Za svaki kljuc program baca error zbog prevelikih brojeva, a ne postoji način da dođemo do orginal vektora.\n",
        "\n",
        "2. Which associations were memorized correctly?\n",
        "\n",
        "Ne možemo doći do njih tako da ni jedna.\n",
        "3. Which associations were not memorized correctly and why?\n",
        "Sve su bile krivo zapamćene jer ne možemo doći do njih.\n",
        "4. How can we fix this problem?\n",
        "Prbacujući matricu u ortonormiranu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E32I0ceyEp6t"
      },
      "outputs": [],
      "source": [
        "print(M)\n",
        "\n",
        "for k in [a1, a2, a3, a4]:\n",
        "  print(k, char(M@k))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OYYbtDDEp6u"
      },
      "source": [
        "### 2.1.4 Input vector orthogonalization\n",
        "\n",
        "In this experiment we show an associative memory, which uses keys that are orthonormalized. We use the Gram-Schmidt orthogonalization method as follows. We first form the matrix $\\mathbf{A}$ using vectors $a_i$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "G2kNjAm6Ep6u"
      },
      "outputs": [],
      "source": [
        "A=np.hstack([a1, a2, a3, a4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJbR4-GWEp6u"
      },
      "source": [
        "After this step we perform the orthonormalization step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f_Agn3fLEp6u"
      },
      "outputs": [],
      "source": [
        "from scipy.linalg import orth\n",
        "C=orth(A.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYzHEQ5aEp6u"
      },
      "source": [
        "We extract individual orthonormal vectors $c_i$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ON4barCfEp6u"
      },
      "outputs": [],
      "source": [
        "c1=np.array([C[0]]).T\n",
        "c2=np.array([C[1]]).T\n",
        "c3=np.array([C[2]]).T\n",
        "c4=np.array([C[3]]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRHUHVz3Ep6v"
      },
      "source": [
        "In the next step we form a new matrix $\\mathbf{M}$ using vectors $c_i$ instead of vectors $a_i$ when creating the matrix $\\mathbf{M}$. Verify the responses of matrix $\\mathbf{M}$ with vectors $c_i$ as inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_oKosYbEp6v"
      },
      "outputs": [],
      "source": [
        "M = b1 * c1.T + b2 * c2.T + b3 * c3.T + b4 * c4.T\n",
        "for c in [c1, c2, c3, c4]:\n",
        "    print(c.T, char(M@c))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0DMp9GgEp6v"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. What is the effect of vector orthonormalization?\n",
        "\n",
        "Tim postupkom vektore u matrici činimo ortogonalnima ćime osiguravamo dobivanje točno pohranjenih podataka. \n",
        "2. How many pairs were correctly memorized?\n",
        "\n",
        "Samo je prvi dio para dobro zapamćen do drugog djela ne možemo doći.\n",
        "3. What can we expect when normalizing the vectors?\n",
        "\n",
        "Možemo očekivati točne rezultate jer se rješavamo šuma u vektorima.\n",
        "4. What can we expect when only orthogonalizing the vectors?\n",
        "\n",
        "Točne rezultate osim ako se šum uvukao u vektore jer će on onda davati pogrešne rezultate.\n",
        "5. What can we expect if vectors $c_i$ are linearly independent but not orthogonal? \n",
        "\n",
        "Da je kapacitet memorije jednak dimenziji vektora i da može doći do pogreške pri pamćenju.\n",
        "\n",
        "### 2.1.5 Finding the correlation matrix using matrix inversion\n",
        "\n",
        "For previously used word pairs (*ruka*-*vrat*, *kset*-*kraj*, *more*-*cres*, *mama*-*otac*) find a $4\\times 4$ correlation matrix $\\mathbf{M}$ as $\\mathbf{M} = \\mathbf{B}\\mathbf{A}^{-1}$, where matrix $\\mathbf{B}$ is defined as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H8mwLLkREp6v"
      },
      "outputs": [],
      "source": [
        "B=np.hstack([b1, b2, b3, b4])\n",
        "M=B@np.linalg.inv(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D6RuYW0Ep6v"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. Were all associations properly memorized? Remark: The result should be rounded to the nearest number before comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJXGVNCsEp6v"
      },
      "outputs": [],
      "source": [
        "for a in [a1, a2, a3, a4]:\n",
        "    print(a.T, char(np.round(M@a)))\n",
        "\n",
        "print()\n",
        "\n",
        "for b in [b1, b2, b3, b4]:\n",
        "    print(b.T, char(np.round(M@b)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dobro su zapamćene samo prve riječi u paru."
      ],
      "metadata": {
        "id": "E7nDkbKS3rPj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTjippiLEp6v"
      },
      "source": [
        "### 2.1.6 Finding the correlation matrix using pseudo-inversion\n",
        "\n",
        "A pseudo-inverse matrix can be used in order to find the correlation matrix when number of associations is larger than dimensionality of vectors representing the associations. In this case, the correlation matrix can be found as $\\mathbf{M} = \\mathbf{B}\\mathbf{A}^{+}$, where $\\mathbf{A}^{+}$ is a pseudo-inverse matrix defined as $\\mathbf{A}^{+} = \\mathbf{A}^{T}(\\mathbf{A}\\mathbf{A}^{T})^{-1}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2LDSeuAEp6w"
      },
      "source": [
        "Assume that the vectors $a_i$ and $b_i$ are defined previously (five associations in total). Find the pseudo-inverse matrix for this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1e82cFMvEp6w"
      },
      "outputs": [],
      "source": [
        "a1 = np.array([[1, 0, 0, 0]]).T\n",
        "a2 = np.array([[0, 1, 0, 0]]).T\n",
        "a3 = np.array([[0, 0, 1, 0]]).T\n",
        "a4 = np.array([[0, 0, 0, 1]]).T\n",
        "\n",
        "a5 = (a1 + a3) / np.sqrt(2)\n",
        "\n",
        "A=np.hstack([a1, a2, a3, a4, a5])\n",
        "B=np.hstack([b1, b2, b3, b4, b5])\n",
        "\n",
        "A_pseudo=A.T@np.linalg.inv(A@A.T)\n",
        "M=B@A_pseudo\n",
        "\n",
        "print(A_pseudo@A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnt6GuSYEp6w"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. Were all pairs memorized correctly?\n",
        "\n",
        "Ponovo je došlo do pogreške u redcima koji su sudjelovali u stvaranju a5 retka kod prve riječi apra, a kod druge ništa nije dobro zapamćeno.\n",
        "2. If not, what is the error between expected and obtained values?\n",
        "\n",
        "Pogreška je u tome što umnožak pseudo A matrice i A matrice ne daje jediničnu matricu, a to je uvjet perfektne asocijacije."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OYQBcF0Ep6w"
      },
      "outputs": [],
      "source": [
        "for a in [a1, a2, a3, a4, a5]:\n",
        "    print(a.T, char(np.round(M@a)))\n",
        "\n",
        "for b in [b1, b2, b3, b4, b5]:\n",
        "    print(b.T, char(np.round(M@b)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hz4kIZ1Ep6w"
      },
      "source": [
        "## 2.2 Finding the correlation matrix using supervised learning\n",
        "\n",
        "This experiment shows us how to form the matrix $\\mathbf{M}$ using supervised learning. In two following experiments we will use learning with error correction.\n",
        "\n",
        "### 2.2.1 Learning with error correction\n",
        "\n",
        "Form matrices $\\mathbf{A}$ and $\\mathbf{B}$ where each contains 4 vectors stacked in columns as explained in previous experiments. Check the contents of obtained matrices with following operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gwp2VHSfEp6w"
      },
      "outputs": [],
      "source": [
        "a1=real(\"ruka\")\n",
        "a2=real(\"kset\")\n",
        "a3=real(\"more\")\n",
        "a4=real(\"mama\")\n",
        "\n",
        "b1=real(\"vrat\")\n",
        "b2=real(\"kraj\")\n",
        "b3=real(\"cres\")\n",
        "b4=real(\"otac\")\n",
        "\n",
        "A=np.hstack([a1, a2, a3, a4])\n",
        "B=np.hstack([b1, b2, b3, b4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1ypxxDxEp6w"
      },
      "source": [
        "In order to start the learning procedure we have to initialize the matrix $\\mathbf{M}$ (For example, random values uniformly generated in $[-0.5, 0.5]$ interval):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NHge9e1LEp6w"
      },
      "outputs": [],
      "source": [
        "M=np.random.rand(4, 4)-0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RJOhxRrEp6x"
      },
      "source": [
        "For the learning part use the function *trainlms*, which is the implementation of the Widrow-Hoff LMS learning algorithm. The function can be used as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "He6K65cLEp6x"
      },
      "outputs": [],
      "source": [
        "def trainlms(A, B, M, ni, max_num_iter, min_err=0.02):\n",
        "    d=B\n",
        "    x=A\n",
        "    w=M\n",
        "    \n",
        "    n=0\n",
        "    err=[]\n",
        "    while (n<max_num_iter):\n",
        "        n+=1\n",
        "        e=d-w@x\n",
        "        w+=ni*np.dot(e, x.T)\n",
        "        err.append(np.sum(np.sum(np.multiply(e, e))))\n",
        "        if (err[-1]<min_err):\n",
        "            break\n",
        "    return w, err"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-tFW4ayEp6x"
      },
      "source": [
        "where *max_num_iter* is the number of iterations and *ni* is the learning rate. Find the *max_num_iter* variable experimentally. For *ni* you can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RS2HWCCDEp6x"
      },
      "outputs": [],
      "source": [
        "ni=0.9999/np.linalg.eig(A @ A.T)[0].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew127WFfEp6x"
      },
      "source": [
        "The function trainlms performs the learning until SSE drops below $0.02$ or maximum number of iterations is performed. After the learning phase, look at the responses of the correlation matrix $\\mathbf{M}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mTEaxzzJEp6x"
      },
      "outputs": [],
      "source": [
        "M, e=trainlms(A, B, M, ni, 100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIq5cHCJEp6x"
      },
      "source": [
        "If we type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcy3gmXrEp6x"
      },
      "outputs": [],
      "source": [
        "np.round(M@A)==B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8fzJvMwEp6y"
      },
      "source": [
        "we will see, which characters were properly reconstructed: the positions with correct reconstructions will have value *True* and  other positions will have value *False*. By calling the *trainlms* multiple times we can extend the learning process and maybe increase the number of memorized characters but the proper way to extend the learning process is to increase the *max_num_iter* variable. We can draw a graph, which plots the error with number of iterations (in logaritmic scale) using the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qwKOshrEp6y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(e)\n",
        "plt.yscale(\"log\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gWBDJb8Ep6y"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. Plot a graph showing number of memorized characters tied to number of used iterations. (Caution: When building the graph, start the simulation with the same starting matrix.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRWcACBEEp6y"
      },
      "outputs": [],
      "source": [
        "num_letters=[]\n",
        "M_initial=np.random.rand(4, 4)-0.5\n",
        "for i in range(1, 100000, 25):\n",
        "    M, e=trainlms(A, B, M_initial, ni, i)\n",
        "    num_letters.append(np.sum(np.round(np.dot(M, A))==B))\n",
        "\n",
        "plt.plot(list(range(1, 100000, 25)), num_letters)\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"Number of correct letters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AayurU-sEp6y"
      },
      "source": [
        "## 2.2.2 Effect of larger number of associations\n",
        "\n",
        "This experiment demonstrates the capacity of the associative memory. What is the capacity of a $4\\times 4$ correlation matrix based associative memory?\n",
        "\n",
        "For additional pair '*auto*'-'*mrak*' create vectors $a_5$ and $b_5$ as explained in the previous part of the exercise. Create new matrices A and B with dimensions $4$ (rows) $\\times$ $5$ (columns) in the same way as previously explained. Initialize the matrix $\\mathbf{M}$ with random starting values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g-Fkb5AZEp6y"
      },
      "outputs": [],
      "source": [
        "a5=real(\"auto\")\n",
        "b5=real(\"mrak\")\n",
        "\n",
        "A=np.hstack([a1, a2, a3, a4, a5])\n",
        "B=np.hstack([b1, b2, b3, b4, b5])\n",
        "\n",
        "M=np.random.rand(4, 4)-0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es0D7a0kEp6y"
      },
      "source": [
        "Use the *trainlms* function in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Wu-SeDSEp6y"
      },
      "outputs": [],
      "source": [
        "ni=0.9999/max(np.linalg.eig(np.dot(A, A.T))[0])\n",
        "M, e=trainlms(A, B, M, ni, 100000)\n",
        "print(np.sum(np.round(np.dot(M, A))==B))\n",
        "print(e[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWtVfThsEp6y"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. How many iterations did you use?\n",
        "\n",
        "Pokušao sam sa 10k, 100k i 1m iteracija da vidim ima li razlike.\n",
        "2. How many characters were memorized correctly?\n",
        "\n",
        "2\n",
        "3. What is the SSE error?\n",
        "\n",
        "Zbroj kvadratnih razlika od prosječne vrijednosti skupa za svaki element, ovdje je ovijek oko 219.\n",
        "4. What happens if we call the function from the beginning?\n",
        "\n",
        "Onda ponovo sa trainlms učimo, ali ne dolazi do drugačijih rezultata.\n",
        "5. How many characters are correctly memorized now and how large is the mistake? Is there any difference and why?\n",
        "\n",
        "Pogreška je još uvijek jednake veličine tj samo su 2 znaka točno memorirana. Razlike nema ni nakon dodatnog učenja jer su vjerojatno dodavanjem dodatnih stupaca izgubljeni neki podatci i s nikakvom količinom učenja nebi ih mogli vratiti.\n",
        "6. Is it possible to train this network in order to memorize all five associations?\n",
        "\n",
        "Ne\n",
        "7. Why? (Explain the previous answer) \n",
        "\n",
        "Dodavanjem dodatnih stupaca i povečavanjem broja vektora iznad postavljene veličine od 4x4 mjenjamo prije upisane informacije i zato ne možemo doći do ispravno zapisanih poruka i karaktera."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}